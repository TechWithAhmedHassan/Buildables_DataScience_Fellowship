{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77804eb7",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Feature Engineering – Task Sheet\n",
    "\n",
    "**Author:** Ahmed Hassan\n",
    "  \n",
    "**Date:** 2025-08-29\n",
    "\n",
    "**Instructions**\n",
    "- Each task appears in this single notebook.\n",
    "- For every task: **Introduction → Data loading & preprocessing steps only → Results & observations**.\n",
    "- Keep things concise. Only minimal visuals where explicitly required (e.g., outlier visualization).\n",
    "- Some datasets (Kaggle) require manual download. The code tries to download from public sources when possible; otherwise, put files under `./data/` as instructed below.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Setup Cell (Run once)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (if needed) and import libraries\n",
    "# If running in an environment where you cannot install packages, comment-out pip lines.\n",
    "# !pip install -q pandas numpy scikit-learn imbalanced-learn scipy matplotlib nltk requests\n",
    "\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, PowerTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.feature_selection import RFE, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK for text preprocessing (Task 12)\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "try:\n",
    "    nltk.data.find(\"wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def try_read_csv(local_name: str, url: str = None, **read_kwargs) -> pd.DataFrame:\n",
    "    # Try loading a CSV from ./data/local_name; if not present and url is provided,\n",
    "    # try to download via pandas.read_csv(url). If both fail, raise a clear error.\n",
    "    local_path = os.path.join(DATA_DIR, local_name)\n",
    "    if os.path.exists(local_path):\n",
    "        return pd.read_csv(local_path, **read_kwargs)\n",
    "    if url:\n",
    "        try:\n",
    "            return pd.read_csv(url, **read_kwargs)\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not download from URL. Place the file at {local_path}. Original error: {e}\"\n",
    "            )\n",
    "    raise FileNotFoundError(\n",
    "        f\"Place the required file at {local_path} or supply a valid URL.\"\n",
    "    )\n",
    "\n",
    "def report_missing(df: pd.DataFrame, title: str = \"Missing Values Report\"):\n",
    "    miss = df.isna().sum().sort_values(ascending=False)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(miss[miss > 0])\n",
    "    if miss.sum() == 0:\n",
    "        print(\"No missing values detected.\")\n",
    "\n",
    "print('Setup complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67b053",
   "metadata": {},
   "source": [
    "## Task 1: Handling Missing Data – Titanic Dataset\n",
    "\n",
    "**Introduction (overview + chosen techniques):**  \n",
    "We identify and treat missing values using mean/median (numeric), mode (categorical), and selective dropping when a column is mostly missing.\n",
    "\n",
    "**Dataset:** Titanic (Kaggle). Fallback: Seaborn's `titanic` sample if available.  \n",
    "**What to show:** Missing counts before/after; dataset shape before/after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1 Code ---\n",
    "# Preferred: Kaggle 'train.csv' under ./data/train.csv (rename to titanic_train.csv)\n",
    "# Fallback: try seaborn built-in if internet available (not guaranteed).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_titanic = None\n",
    "local_path = os.path.join(DATA_DIR, \"titanic_train.csv\")  # rename your Kaggle train.csv to titanic_train.csv\n",
    "\n",
    "if os.path.exists(local_path):\n",
    "    df_titanic = pd.read_csv(local_path)\n",
    "else:\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        df_titanic = sns.load_dataset(\"titanic\")  # schema differs slightly\n",
    "    except Exception:\n",
    "        raise FileNotFoundError(\n",
    "            \"Provide Titanic train.csv as ./data/titanic_train.csv or ensure seaborn can load the sample dataset.\"\n",
    "        )\n",
    "\n",
    "print(\"Shape before:\", df_titanic.shape)\n",
    "report_missing(df_titanic, \"Titanic Missing Before\")\n",
    "\n",
    "# Separate numeric and categorical\n",
    "num_cols = df_titanic.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df_titanic.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Impute numeric: median\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "df_titanic[num_cols] = num_imputer.fit_transform(df_titanic[num_cols])\n",
    "\n",
    "# Impute categorical: most_frequent\n",
    "if cat_cols:\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    df_titanic[cat_cols] = cat_imputer.fit_transform(df_titanic[cat_cols])\n",
    "\n",
    "report_missing(df_titanic, \"Titanic Missing After\")\n",
    "print(\"Shape after:\", df_titanic.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a465800",
   "metadata": {},
   "source": [
    "## Task 2: Encoding Categorical Variables – Car Evaluation Dataset\n",
    "\n",
    "**Introduction:** Categorical variables must be encoded to numeric form. We compare **Label Encoding** (ordinal codes) vs **One-Hot Encoding** (binary indicators).\n",
    "\n",
    "**Dataset:** UCI Car Evaluation.  \n",
    "**What to show:** Shapes and sample heads for both encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e57ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 2 Code ---\n",
    "uci_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "cols = [\"buying\",\"maint\",\"doors\",\"persons\",\"lug_boot\",\"safety\",\"class\"]\n",
    "\n",
    "car_df = try_read_csv(\"car.data\", uci_url, header=None, names=cols)\n",
    "\n",
    "# Label Encoding (including target for demonstration)\n",
    "le = LabelEncoder()\n",
    "car_le = car_df.apply(le.fit_transform)\n",
    "\n",
    "print(\"Label Encoded shape:\", car_le.shape)\n",
    "print(car_le.head())\n",
    "\n",
    "# One-hot Encoding (exclude target when appropriate; here we encode all features, keep target separate)\n",
    "X = car_df.drop(columns=[\"class\"])\n",
    "y = car_df[\"class\"]\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "X_ohe = pd.DataFrame(ohe.fit_transform(X), columns=ohe.get_feature_names_out(X.columns))\n",
    "\n",
    "print(\"One-hot Encoded shape:\", X_ohe.shape)\n",
    "print(X_ohe.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181a8c0",
   "metadata": {},
   "source": [
    "## Task 3: Feature Scaling – Wine Quality Dataset\n",
    "\n",
    "**Introduction:** Compare **Normalization (MinMaxScaler)** and **Standardization (StandardScaler)** on numeric features.\n",
    "\n",
    "**Dataset:** UCI Wine Quality (red wine).  \n",
    "**What to show:** Summary statistics before/after; basic distribution change (no heavy plots).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 3 Code ---\n",
    "wine_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine = try_read_csv(\"winequality-red.csv\", wine_url, sep=\";\")\n",
    "\n",
    "features = wine.drop(columns=[\"quality\"])\n",
    "minmax = MinMaxScaler()\n",
    "std = StandardScaler()\n",
    "\n",
    "features_minmax = pd.DataFrame(minmax.fit_transform(features), columns=features.columns)\n",
    "features_std = pd.DataFrame(std.fit_transform(features), columns=features.columns)\n",
    "\n",
    "print(\"Original describe:\\n\", features.describe().T[[\"mean\",\"std\",\"min\",\"max\"]])\n",
    "print(\"\\nMinMax describe:\\n\", features_minmax.describe().T[[\"mean\",\"std\",\"min\",\"max\"]])\n",
    "print(\"\\nStandardized describe:\\n\", features_std.describe().T[[\"mean\",\"std\",\"min\",\"max\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a6d9a",
   "metadata": {},
   "source": [
    "## Task 4: Handling Outliers – Boston Housing Dataset\n",
    "\n",
    "**Introduction:** Detect outliers via **Z-score** and **IQR**. Provide simple visual (boxplot) as required.\n",
    "\n",
    "**Dataset:** Boston Housing (UCI).  \n",
    "**What to show:** Counts removed via each method; shape before/after; one boxplot for a representative column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5df986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 4 Code ---\n",
    "boston_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "boston_cols = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
    "\n",
    "boston = try_read_csv(\"housing.data\", boston_url, delim_whitespace=True, header=None, names=boston_cols)\n",
    "\n",
    "print(\"Shape before:\", boston.shape)\n",
    "\n",
    "# Simple boxplot for 'RM'\n",
    "plt.figure()\n",
    "boston[\"RM\"].plot(kind=\"box\", title=\"Boxplot of RM (rooms)\")\n",
    "plt.show()\n",
    "\n",
    "# Z-score method (remove rows where any feature has |z| > 3)\n",
    "z = np.abs(stats.zscore(boston.select_dtypes(include=[np.number])))\n",
    "z_mask = (z < 3).all(axis=1)\n",
    "boston_z = boston[z_mask]\n",
    "print(\"After Z-score filter (|z|<3):\", boston_z.shape)\n",
    "\n",
    "# IQR method for LSTAT (example feature)\n",
    "Q1 = boston[\"LSTAT\"].quantile(0.25)\n",
    "Q3 = boston[\"LSTAT\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "mask_iqr = (boston[\"LSTAT\"] >= Q1 - 1.5*IQR) & (boston[\"LSTAT\"] <= Q3 + 1.5*IQR)\n",
    "boston_iqr = boston[mask_iqr]\n",
    "print(\"After IQR filter on LSTAT:\", boston_iqr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae02191",
   "metadata": {},
   "source": [
    "## Task 5: Advanced Data Imputation – Retail Sales Dataset\n",
    "\n",
    "**Introduction:** Use **KNNImputer** and **MICE (IterativeImputer)** for smarter imputations that consider feature relationships.\n",
    "\n",
    "**Dataset:** Retail Sales (Kaggle – provide your CSV as `retail_sales.csv`).  \n",
    "**What to show:** Missing values before/after for each technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b038a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 5 Code ---\n",
    "retail_path = os.path.join(DATA_DIR, \"retail_sales.csv\")\n",
    "if os.path.exists(retail_path):\n",
    "    retail = pd.read_csv(retail_path)\n",
    "else:\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(\"2021-01-01\", periods=200, freq=\"D\")\n",
    "    retail = pd.DataFrame({\n",
    "        \"date\": np.random.choice(dates, size=500),\n",
    "        \"store\": np.random.choice([\"A\",\"B\",\"C\"], size=500),\n",
    "        \"item\": np.random.choice([\"SKU1\",\"SKU2\",\"SKU3\",\"SKU4\"], size=500),\n",
    "        \"price\": np.random.uniform(5, 50, size=500),\n",
    "        \"promo\": np.random.choice([0,1], size=500, p=[0.7, 0.3]),\n",
    "        \"sales\": np.random.poisson(20, size=500).astype(float)\n",
    "    })\n",
    "    # Inject missingness\n",
    "    for col in [\"price\",\"promo\",\"sales\"]:\n",
    "        idx = np.random.choice(retail.index, size=60, replace=False)\n",
    "        retail.loc[idx, col] = np.nan\n",
    "\n",
    "print(\"Missing BEFORE:\")\n",
    "report_missing(retail)\n",
    "\n",
    "# Encode categorical to numeric for imputation\n",
    "retail_enc = retail.copy()\n",
    "for c in retail_enc.select_dtypes(exclude=[np.number]).columns:\n",
    "    retail_enc[c] = LabelEncoder().fit_transform(retail_enc[c].astype(str))\n",
    "\n",
    "num_cols = retail_enc.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# KNN Imputer\n",
    "knn_imp = KNNImputer(n_neighbors=5)\n",
    "retail_knn = retail_enc.copy()\n",
    "retail_knn[num_cols] = knn_imp.fit_transform(retail_knn[num_cols])\n",
    "\n",
    "print(\"\\nMissing AFTER KNN:\")\n",
    "report_missing(retail_knn)\n",
    "\n",
    "# MICE / Iterative Imputer\n",
    "mice_imp = IterativeImputer(random_state=42, sample_posterior=False, max_iter=10)\n",
    "retail_mice = retail_enc.copy()\n",
    "retail_mice[num_cols] = mice_imp.fit_transform(retail_mice[num_cols])\n",
    "\n",
    "print(\"\\nMissing AFTER MICE:\")\n",
    "report_missing(retail_mice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbd3a6",
   "metadata": {},
   "source": [
    "## Task 6: Feature Engineering – Heart Disease Dataset\n",
    "\n",
    "**Introduction:** Create useful derived features such as age groups, cholesterol categories, and composite risk flags.\n",
    "\n",
    "**Dataset:** UCI Heart Disease.  \n",
    "**What to show:** `head()` with new features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 6 Code ---\n",
    "uci_heart_url = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/heart/heart.csv\"\n",
    "heart = try_read_csv(\"heart.csv\", uci_heart_url)\n",
    "\n",
    "# Feature Engineering\n",
    "heart[\"age_group\"] = pd.cut(heart[\"age\"], bins=[0,40,55,70,120],\n",
    "                            labels=[\"<=40\",\"41-55\",\"56-70\",\">70\"], right=True, include_lowest=True)\n",
    "\n",
    "heart[\"chol_cat\"] = pd.cut(heart[\"chol\"], bins=[0,200,240,600],\n",
    "                           labels=[\"desirable\",\"borderline_high\",\"high\"], include_lowest=True)\n",
    "\n",
    "# Simplified risk score example\n",
    "heart[\"risk_bp_chol\"] = ((heart[\"trestbps\"] > 130).astype(int) + (heart[\"chol\"] > 240).astype(int))\n",
    "\n",
    "# Composite risk flag example\n",
    "heart[\"risk_flag\"] = (\n",
    "    ((heart[\"age\"] > 55).astype(int) +\n",
    "     (heart[\"trestbps\"] > 140).astype(int) +\n",
    "     (heart[\"chol\"] > 240).astype(int) +\n",
    "     (heart[\"exang\"] == 1).astype(int)) >= 2\n",
    ").astype(int)\n",
    "\n",
    "print(heart[[\"age\",\"trestbps\",\"chol\",\"age_group\",\"chol_cat\",\"risk_bp_chol\",\"risk_flag\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb7010",
   "metadata": {},
   "source": [
    "## Task 7: Variable Transformation – Bike Sharing Dataset\n",
    "\n",
    "**Introduction:** Reduce skewness using **log**, **square-root**, and **Box-Cox** transforms.\n",
    "\n",
    "**Dataset:** UCI Bike Sharing (day.csv).  \n",
    "**What to show:** Skewness before/after for key skewed columns (e.g., `cnt`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e91fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 7 Code ---\n",
    "bike_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\"\n",
    "\n",
    "# Try to download and read 'day.csv' from the zip; else expect ./data/day.csv\n",
    "day_path = os.path.join(DATA_DIR, \"day.csv\")\n",
    "if not os.path.exists(day_path):\n",
    "    try:\n",
    "        import requests\n",
    "        from io import BytesIO\n",
    "        resp = requests.get(bike_url, timeout=30)\n",
    "        zf = zipfile.ZipFile(BytesIO(resp.content))\n",
    "        with zf.open(\"day.csv\") as f:\n",
    "            day = pd.read_csv(f)\n",
    "    except Exception:\n",
    "        day = try_read_csv(\"day.csv\")\n",
    "else:\n",
    "    day = pd.read_csv(day_path)\n",
    "\n",
    "def skew_report(s, name):\n",
    "    print(f\"{name} skew: {s.skew():.4f}\")\n",
    "\n",
    "skew_report(day[\"cnt\"], \"Original cnt\")\n",
    "\n",
    "# Log (use log1p to handle zeros)\n",
    "day[\"cnt_log\"] = np.log1p(day[\"cnt\"])\n",
    "skew_report(day[\"cnt_log\"], \"Log cnt\")\n",
    "\n",
    "# Square-root\n",
    "day[\"cnt_sqrt\"] = np.sqrt(day[\"cnt\"])\n",
    "skew_report(day[\"cnt_sqrt\"], \"Sqrt cnt\")\n",
    "\n",
    "# Box-Cox (requires strictly positive; cnt >= 0, add +1)\n",
    "pt = PowerTransformer(method=\"box-cox\", standardize=False)\n",
    "day[\"cnt_boxcox\"] = pt.fit_transform((day[\"cnt\"] + 1).values.reshape(-1,1)).ravel()\n",
    "skew_report(day[\"cnt_boxcox\"], \"Box-Cox cnt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685394d4",
   "metadata": {},
   "source": [
    "## Task 8: Feature Selection – Diabetes Dataset\n",
    "\n",
    "**Introduction:** Select important features via **Correlation**, **Mutual Information**, and **RFE**.\n",
    "\n",
    "**Dataset:** Pima Indians Diabetes (provide `diabetes.csv` locally or use mirror).  \n",
    "**What to show:** Selected features from each method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b721c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 8 Code ---\n",
    "pima_url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "pima_cols = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"]\n",
    "\n",
    "diabetes = try_read_csv(\"diabetes.csv\", pima_url, header=None, names=pima_cols)\n",
    "\n",
    "X = diabetes.drop(columns=[\"Outcome\"])\n",
    "y = diabetes[\"Outcome\"]\n",
    "\n",
    "# 1) Correlation (absolute)\n",
    "corr = X.join(y).corr(numeric_only=True)[\"Outcome\"].drop(\"Outcome\").abs().sort_values(ascending=False)\n",
    "print(\"Correlation with Outcome:\\n\", corr)\n",
    "\n",
    "# 2) Mutual Information\n",
    "mi = mutual_info_classif(X, y, random_state=42)\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nMutual Information:\\n\", mi_series)\n",
    "\n",
    "# 3) RFE with Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "rfe = RFE(lr, n_features_to_select=5)\n",
    "rfe.fit(X, y)\n",
    "selected_rfe = X.columns[rfe.support_]\n",
    "print(\"\\nRFE Selected (5):\", list(selected_rfe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79622ab2",
   "metadata": {},
   "source": [
    "## Task 9: Handling Imbalanced Data – Credit Card Fraud Detection\n",
    "\n",
    "**Introduction:** Demonstrate resampling using **SMOTE**, **ADASYN**, and **undersampling**.\n",
    "\n",
    "**Dataset:** Kaggle Credit Card Fraud (`creditcard.csv`).  \n",
    "**What to show:** Class distribution before/after each technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 9 Code ---\n",
    "path_cc = os.path.join(DATA_DIR, \"creditcard.csv\")\n",
    "if os.path.exists(path_cc):\n",
    "    cc = pd.read_csv(path_cc)\n",
    "    X = cc.drop(columns=[\"Class\"])\n",
    "    y = cc[\"Class\"]\n",
    "else:\n",
    "    from sklearn.datasets import make_classification\n",
    "    X, y = make_classification(n_samples=5000, n_features=20, n_informative=3, n_redundant=2,\n",
    "                               weights=[0.98, 0.02], random_state=42)\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(y, name=\"Class\")\n",
    "\n",
    "print(\"Class distribution BEFORE:\\n\", y.value_counts(normalize=True))\n",
    "\n",
    "# SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_sm, y_sm = sm.fit_resample(X, y)\n",
    "print(\"\\nAFTER SMOTE:\\n\", y_sm.value_counts(normalize=True))\n",
    "\n",
    "# ADASYN\n",
    "ad = ADASYN(random_state=42)\n",
    "X_ad, y_ad = ad.fit_resample(X, y)\n",
    "print(\"\\nAFTER ADASYN:\\n\", y_ad.value_counts(normalize=True))\n",
    "\n",
    "# Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_ru, y_ru = rus.fit_resample(X, y)\n",
    "print(\"\\nAFTER Random Undersampling:\\n\", y_ru.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d143c53",
   "metadata": {},
   "source": [
    "## Task 10: Combining Multiple Datasets – MovieLens Dataset\n",
    "\n",
    "**Introduction:** Merge ratings, movies, and (if available) users/links.\n",
    "\n",
    "**Dataset:** MovieLens (e.g., **ml-latest-small**).  \n",
    "**What to show:** Merged dataset shape and `head()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339409f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 10 Code ---\n",
    "ml_dir = os.path.join(DATA_DIR, \"ml-latest-small\")\n",
    "ratings_path = os.path.join(ml_dir, \"ratings.csv\")\n",
    "movies_path = os.path.join(ml_dir, \"movies.csv\")\n",
    "links_path = os.path.join(ml_dir, \"links.csv\")\n",
    "\n",
    "if not (os.path.exists(ratings_path) and os.path.exists(movies_path)):\n",
    "    # Try download\n",
    "    try:\n",
    "        import requests\n",
    "        url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "        r = requests.get(url, timeout=30)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(DATA_DIR)\n",
    "    except Exception:\n",
    "        print(\"Please place MovieLens 'ml-latest-small' in ./data/\")\n",
    "        \n",
    "ratings = pd.read_csv(ratings_path) if os.path.exists(ratings_path) else None\n",
    "movies = pd.read_csv(movies_path) if os.path.exists(movies_path) else None\n",
    "links  = pd.read_csv(links_path) if os.path.exists(links_path) else None\n",
    "\n",
    "if ratings is None or movies is None:\n",
    "    raise FileNotFoundError(\"ratings.csv/movies.csv not found. Ensure MovieLens is available in ./data/.\")\n",
    "\n",
    "merged = ratings.merge(movies, on=\"movieId\", how=\"left\")\n",
    "if links is not None:\n",
    "    merged = merged.merge(links, on=[\"movieId\"], how=\"left\")\n",
    "\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50419980",
   "metadata": {},
   "source": [
    "## Task 11: Dimensionality Reduction – MNIST Dataset\n",
    "\n",
    "**Introduction:** Reduce dimensionality with **PCA** (retain 95% variance) and visualize a basic **t-SNE** 2D embedding (no labels/insights beyond shape print).\n",
    "\n",
    "**Dataset:** MNIST (via `keras.datasets.mnist`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae054ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 11 Code ---\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# Flatten\n",
    "X_flat = X.reshape((X.shape[0], -1)).astype(\"float32\") / 255.0\n",
    "print(\"MNIST flattened shape:\", X_flat.shape)\n",
    "\n",
    "# PCA retain 95% variance\n",
    "pca = PCA(n_components=0.95, svd_solver=\"full\", random_state=42)\n",
    "X_pca = pca.fit_transform(X_flat)\n",
    "print(\"PCA shape (95% var):\", X_pca.shape)\n",
    "\n",
    "# t-SNE (subset for speed)\n",
    "idx = np.random.choice(len(X_pca), size=5000, replace=False)\n",
    "tsne = TSNE(n_components=2, init=\"random\", random_state=42, perplexity=30, learning_rate=\"auto\")\n",
    "X_tsne = tsne.fit_transform(X_pca[idx])\n",
    "\n",
    "print(\"t-SNE 2D embedding shape:\", X_tsne.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6a323",
   "metadata": {},
   "source": [
    "## Task 12: Text Preprocessing – IMDB Movie Reviews Dataset\n",
    "\n",
    "**Introduction:** Clean and preprocess raw text: lowercasing, stopword removal, tokenization, stemming/lemmatization.\n",
    "\n",
    "**Dataset:** Provide a CSV `IMDB_Dataset.csv` with columns `review` and `sentiment` (Kaggle format).  \n",
    "**What to show:** Example before/after for 3 reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba90d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 12 Code ---\n",
    "path_imdb = os.path.join(DATA_DIR, \"IMDB_Dataset.csv\")\n",
    "if not os.path.exists(path_imdb):\n",
    "    raise FileNotFoundError(\"Place IMDB_Dataset.csv in ./data/ with columns: review, sentiment\")\n",
    "\n",
    "imdb = pd.read_csv(path_imdb)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(s: str, use_stem=False, use_lemma=True):\n",
    "    s = str(s).lower()\n",
    "    tokens = word_tokenize(s)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    if use_stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    if use_lemma:\n",
    "        tokens = [lemm.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Show 3 examples before/after\n",
    "sample = imdb.sample(3, random_state=42).copy()\n",
    "sample[\"cleaned\"] = sample[\"review\"].apply(preprocess_text)\n",
    "print(sample[[\"review\",\"cleaned\",\"sentiment\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086b62a",
   "metadata": {},
   "source": [
    "## Task 13: Time-Series Preprocessing – Air Quality Dataset\n",
    "\n",
    "**Introduction:** Handle missing timestamps, resample, and smooth.\n",
    "\n",
    "**Dataset:** UCI Air Quality (place `AirQualityUCI.csv` or similar in `./data/`).  \n",
    "**What to show:** Head of the processed series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 13 Code ---\n",
    "aq_local = os.path.join(DATA_DIR, \"AirQualityUCI.csv\")\n",
    "if os.path.exists(aq_local):\n",
    "    aq = pd.read_csv(aq_local, sep=\";\", decimal=\",\")\n",
    "    aq = aq.dropna(axis=1, how=\"all\")\n",
    "    if \"Date\" in aq.columns and \"Time\" in aq.columns:\n",
    "        dt = pd.to_datetime(aq[\"Date\"] + \" \" + aq[\"Time\"], errors=\"coerce\", dayfirst=True)\n",
    "        aq = aq.assign(datetime=dt).drop(columns=[\"Date\",\"Time\"])\n",
    "    elif \"datetime\" in aq.columns:\n",
    "        aq[\"datetime\"] = pd.to_datetime(aq[\"datetime\"], errors=\"coerce\")\n",
    "    else:\n",
    "        raise ValueError(\"Provide Date/Time or datetime columns.\")\n",
    "else:\n",
    "    # Synthesize hourly data with gaps for demonstration\n",
    "    rng = pd.date_range(\"2022-01-01\", periods=500, freq=\"H\")\n",
    "    aq = pd.DataFrame({\"datetime\": rng, \"CO(GT)\": np.random.normal(1.2, 0.3, size=len(rng))})\n",
    "    drop_idx = np.random.choice(aq.index, size=50, replace=False)\n",
    "    aq = aq.drop(index=drop_idx).sort_values(\"datetime\")\n",
    "\n",
    "# Set index and reindex to continuous hourly frequency\n",
    "aq = aq.set_index(\"datetime\").sort_index()\n",
    "full_index = pd.date_range(aq.index.min(), aq.index.max(), freq=\"H\")\n",
    "aq_reindexed = aq.reindex(full_index)\n",
    "\n",
    "# Impute missing via forward-fill then mean fill\n",
    "aq_filled = aq_reindexed.ffill().fillna(aq_reindexed.mean(numeric_only=True))\n",
    "\n",
    "# Resample daily mean\n",
    "aq_daily = aq_filled.resample(\"D\").mean(numeric_only=True)\n",
    "\n",
    "# Smooth with 7-day rolling mean\n",
    "aq_smoothed = aq_daily.rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "print(\"Original head:\\n\", aq.head())\n",
    "print(\"\\nDaily head:\\n\", aq_daily.head())\n",
    "print(\"\\nSmoothed head:\\n\", aq_smoothed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777bd44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ✅ Notes\n",
    "- Where internet downloads are not possible, place the required files in `./data/` as indicated in each task.\n",
    "- Keep the notebook outputs minimal: only the required prints/one simple plot for outliers.\n",
    "- After running all cells, push this `.ipynb` to your GitHub repo.\n",
    "\n",
    "Good luck!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
